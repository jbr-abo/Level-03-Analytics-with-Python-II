{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Name:** Omitted \n",
    "\n",
    "**Student ID:** Omitted\n",
    "\n",
    "## Assignment 02, Question: 03"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Text analysis\n",
    "\n",
    "**In this exercise you will have to work with the data provided in the file 'text data.csv'. The data contains information on 165 articles from the San Francisco Chronicle from the Biz & Tech, Food, and US & World categories.** \n",
    "\n",
    "**You can find the following information:**\n",
    "\n",
    "- **date:** date when the article appeared online\n",
    "- **text:** the content of the article\n",
    "- **title:** the title of the article\n",
    "- **category:** the category of the article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import nltk ## Library for Natural Language Processing\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize  ## We need these to cut our texts into meaningful units.\n",
    "\n",
    "nltk.download('punkt') ## A resource for performing text processing\n",
    "\n",
    "## We also need to exclude 'stopwords' and for this we need the followings-\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>title</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Feb. 13, 2019</td>\n",
       "      <td>While millions more Pacific Gas and Electric C...</td>\n",
       "      <td>PG&amp;E says it’s still trying to limit power shu...</td>\n",
       "      <td>Biz+Tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Feb. 13, 2019</td>\n",
       "      <td>SINGAPORE (AP) — Asian stocks were mostly lowe...</td>\n",
       "      <td>Asian shares retreat as China, US begin trade ...</td>\n",
       "      <td>Biz+Tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Feb. 16, 2018</td>\n",
       "      <td>If anyone knew the ropes about Airbnb rentals,...</td>\n",
       "      <td>Long, winding road to SF’s get-tough registrat...</td>\n",
       "      <td>Biz+Tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Feb. 11, 2019</td>\n",
       "      <td>Half of the PG&amp;E Corp. board of directors will...</td>\n",
       "      <td>PG&amp;E to replace half of its board in wake of b...</td>\n",
       "      <td>Biz+Tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Feb. 15, 2019</td>\n",
       "      <td>A: Real estate today moves at the speed of inf...</td>\n",
       "      <td>What’s a piece of technology you can’t imagine...</td>\n",
       "      <td>Biz+Tech</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            date                                               text  \\\n",
       "0  Feb. 13, 2019  While millions more Pacific Gas and Electric C...   \n",
       "1  Feb. 13, 2019  SINGAPORE (AP) — Asian stocks were mostly lowe...   \n",
       "2  Feb. 16, 2018  If anyone knew the ropes about Airbnb rentals,...   \n",
       "3  Feb. 11, 2019  Half of the PG&E Corp. board of directors will...   \n",
       "4  Feb. 15, 2019  A: Real estate today moves at the speed of inf...   \n",
       "\n",
       "                                               title  category  \n",
       "0  PG&E says it’s still trying to limit power shu...  Biz+Tech  \n",
       "1  Asian shares retreat as China, US begin trade ...  Biz+Tech  \n",
       "2  Long, winding road to SF’s get-tough registrat...  Biz+Tech  \n",
       "3  PG&E to replace half of its board in wake of b...  Biz+Tech  \n",
       "4  What’s a piece of technology you can’t imagine...  Biz+Tech  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Reading the given 'text_dat' csv file\n",
    "\n",
    "df_text = pd.read_csv('text_data.csv')\n",
    "df_text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **In the analysis, you have to work with the column text. Perform basic text preprocessing using stemming. to obtain the most frequent words across all the articles. Try to iterate it at least two times, and in each iteration you should extend the set of stopwords with new ones based on the words you obtained as frequently occurring but are not particularly informative when you try to understand what the news is about. What are the 15 most frequent words after 2 iterations of removing various stopwords?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>title</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Feb. 13, 2019</td>\n",
       "      <td>while millions more pacific gas and electric c...</td>\n",
       "      <td>PG&amp;E says it’s still trying to limit power shu...</td>\n",
       "      <td>Biz+Tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Feb. 13, 2019</td>\n",
       "      <td>singapore (ap) — asian stocks were mostly lowe...</td>\n",
       "      <td>Asian shares retreat as China, US begin trade ...</td>\n",
       "      <td>Biz+Tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Feb. 16, 2018</td>\n",
       "      <td>if anyone knew the ropes about airbnb rentals,...</td>\n",
       "      <td>Long, winding road to SF’s get-tough registrat...</td>\n",
       "      <td>Biz+Tech</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            date                                               text  \\\n",
       "0  Feb. 13, 2019  while millions more pacific gas and electric c...   \n",
       "1  Feb. 13, 2019  singapore (ap) — asian stocks were mostly lowe...   \n",
       "2  Feb. 16, 2018  if anyone knew the ropes about airbnb rentals,...   \n",
       "\n",
       "                                               title  category  \n",
       "0  PG&E says it’s still trying to limit power shu...  Biz+Tech  \n",
       "1  Asian shares retreat as China, US begin trade ...  Biz+Tech  \n",
       "2  Long, winding road to SF’s get-tough registrat...  Biz+Tech  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Performing basic text preprocessing-\n",
    "\n",
    "## We will, first, change the strings of our 'text' column to lower case-\n",
    "df_text['text'] = df_text['text'].str.lower()\n",
    "df_text.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>title</th>\n",
       "      <th>category</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Feb. 13, 2019</td>\n",
       "      <td>while millions more pacific gas and electric c...</td>\n",
       "      <td>PG&amp;E says it’s still trying to limit power shu...</td>\n",
       "      <td>Biz+Tech</td>\n",
       "      <td>[while, millions, more, pacific, gas, and, ele...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Feb. 13, 2019</td>\n",
       "      <td>singapore (ap) — asian stocks were mostly lowe...</td>\n",
       "      <td>Asian shares retreat as China, US begin trade ...</td>\n",
       "      <td>Biz+Tech</td>\n",
       "      <td>[singapore, (ap), —, asian, stocks, were, most...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Feb. 16, 2018</td>\n",
       "      <td>if anyone knew the ropes about airbnb rentals,...</td>\n",
       "      <td>Long, winding road to SF’s get-tough registrat...</td>\n",
       "      <td>Biz+Tech</td>\n",
       "      <td>[if, anyone, knew, the, ropes, about, airbnb, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            date                                               text  \\\n",
       "0  Feb. 13, 2019  while millions more pacific gas and electric c...   \n",
       "1  Feb. 13, 2019  singapore (ap) — asian stocks were mostly lowe...   \n",
       "2  Feb. 16, 2018  if anyone knew the ropes about airbnb rentals,...   \n",
       "\n",
       "                                               title  category  \\\n",
       "0  PG&E says it’s still trying to limit power shu...  Biz+Tech   \n",
       "1  Asian shares retreat as China, US begin trade ...  Biz+Tech   \n",
       "2  Long, winding road to SF’s get-tough registrat...  Biz+Tech   \n",
       "\n",
       "                                               split  \n",
       "0  [while, millions, more, pacific, gas, and, ele...  \n",
       "1  [singapore, (ap), —, asian, stocks, were, most...  \n",
       "2  [if, anyone, knew, the, ropes, about, airbnb, ...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Splitting the texts in text column and adding the result to a new column- 'split'-\n",
    "\n",
    "df_text['split'] = df_text['text'].apply(lambda x: x.split())  \n",
    "df_text.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Importing the PorterStemmer-\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()  ## Creating an object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>title</th>\n",
       "      <th>category</th>\n",
       "      <th>stemmed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Feb. 13, 2019</td>\n",
       "      <td>while millions more pacific gas and electric c...</td>\n",
       "      <td>PG&amp;E says it’s still trying to limit power shu...</td>\n",
       "      <td>Biz+Tech</td>\n",
       "      <td>[while, million, more, pacif, ga, and, electr,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Feb. 13, 2019</td>\n",
       "      <td>singapore (ap) — asian stocks were mostly lowe...</td>\n",
       "      <td>Asian shares retreat as China, US begin trade ...</td>\n",
       "      <td>Biz+Tech</td>\n",
       "      <td>[singapor, (ap), —, asian, stock, were, mostli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Feb. 16, 2018</td>\n",
       "      <td>if anyone knew the ropes about airbnb rentals,...</td>\n",
       "      <td>Long, winding road to SF’s get-tough registrat...</td>\n",
       "      <td>Biz+Tech</td>\n",
       "      <td>[if, anyon, knew, the, rope, about, airbnb, re...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            date                                               text  \\\n",
       "0  Feb. 13, 2019  while millions more pacific gas and electric c...   \n",
       "1  Feb. 13, 2019  singapore (ap) — asian stocks were mostly lowe...   \n",
       "2  Feb. 16, 2018  if anyone knew the ropes about airbnb rentals,...   \n",
       "\n",
       "                                               title  category  \\\n",
       "0  PG&E says it’s still trying to limit power shu...  Biz+Tech   \n",
       "1  Asian shares retreat as China, US begin trade ...  Biz+Tech   \n",
       "2  Long, winding road to SF’s get-tough registrat...  Biz+Tech   \n",
       "\n",
       "                                             stemmed  \n",
       "0  [while, million, more, pacif, ga, and, electr,...  \n",
       "1  [singapor, (ap), —, asian, stock, were, mostli...  \n",
       "2  [if, anyon, knew, the, rope, about, airbnb, re...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_text['stemmed'] = df_text['split'].apply(lambda x: [stemmer.stem(y) for y in x])\n",
    "df_text = df_text.drop(columns=['split'])  ## Dropping the split column\n",
    "df_text.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>title</th>\n",
       "      <th>category</th>\n",
       "      <th>stemmed</th>\n",
       "      <th>split_final</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Feb. 13, 2019</td>\n",
       "      <td>while millions more pacific gas and electric c...</td>\n",
       "      <td>PG&amp;E says it’s still trying to limit power shu...</td>\n",
       "      <td>Biz+Tech</td>\n",
       "      <td>[while, million, more, pacif, ga, and, electr,...</td>\n",
       "      <td>while million more pacif ga and electr co. cus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Feb. 13, 2019</td>\n",
       "      <td>singapore (ap) — asian stocks were mostly lowe...</td>\n",
       "      <td>Asian shares retreat as China, US begin trade ...</td>\n",
       "      <td>Biz+Tech</td>\n",
       "      <td>[singapor, (ap), —, asian, stock, were, mostli...</td>\n",
       "      <td>singapor (ap) — asian stock were mostli lower ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Feb. 16, 2018</td>\n",
       "      <td>if anyone knew the ropes about airbnb rentals,...</td>\n",
       "      <td>Long, winding road to SF’s get-tough registrat...</td>\n",
       "      <td>Biz+Tech</td>\n",
       "      <td>[if, anyon, knew, the, rope, about, airbnb, re...</td>\n",
       "      <td>if anyon knew the rope about airbnb rentals, d...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            date                                               text  \\\n",
       "0  Feb. 13, 2019  while millions more pacific gas and electric c...   \n",
       "1  Feb. 13, 2019  singapore (ap) — asian stocks were mostly lowe...   \n",
       "2  Feb. 16, 2018  if anyone knew the ropes about airbnb rentals,...   \n",
       "\n",
       "                                               title  category  \\\n",
       "0  PG&E says it’s still trying to limit power shu...  Biz+Tech   \n",
       "1  Asian shares retreat as China, US begin trade ...  Biz+Tech   \n",
       "2  Long, winding road to SF’s get-tough registrat...  Biz+Tech   \n",
       "\n",
       "                                             stemmed  \\\n",
       "0  [while, million, more, pacif, ga, and, electr,...   \n",
       "1  [singapor, (ap), —, asian, stock, were, mostli...   \n",
       "2  [if, anyon, knew, the, rope, about, airbnb, re...   \n",
       "\n",
       "                                         split_final  \n",
       "0  while million more pacif ga and electr co. cus...  \n",
       "1  singapor (ap) — asian stock were mostli lower ...  \n",
       "2  if anyon knew the rope about airbnb rentals, d...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Getting back to the single string format under a new column- 'split_final'-\n",
    "\n",
    "df_text['split_final'] = df_text['stemmed'].apply(lambda x: ' '.join(x))\n",
    "df_text.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Importing the library\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer  \n",
    "\n",
    "vect = CountVectorizer()   ## Creating the object\n",
    "\n",
    "text_count = vect.fit_transform(df_text['split_final'])  ## Fitting the data or Creating the representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>000</th>\n",
       "      <th>0007</th>\n",
       "      <th>0071</th>\n",
       "      <th>0072</th>\n",
       "      <th>01</th>\n",
       "      <th>0100</th>\n",
       "      <th>0109when</th>\n",
       "      <th>015</th>\n",
       "      <th>017</th>\n",
       "      <th>0181</th>\n",
       "      <th>...</th>\n",
       "      <th>zip</th>\n",
       "      <th>zone</th>\n",
       "      <th>zones</th>\n",
       "      <th>zoraida</th>\n",
       "      <th>zour</th>\n",
       "      <th>zucchini</th>\n",
       "      <th>zuckerberg</th>\n",
       "      <th>zuni</th>\n",
       "      <th>zuppa</th>\n",
       "      <th>échezeaux</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 15636 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   000  0007  0071  0072  01  0100  0109when  015  017  0181  ...  zip  zone  \\\n",
       "0    1     0     0     0   0     0         0    0    0     0  ...    0     1   \n",
       "1    0     0     0     0   0     0         0    0    0     0  ...    0     0   \n",
       "2    0     0     0     0   0     0         0    0    0     0  ...    0     0   \n",
       "\n",
       "   zones  zoraida  zour  zucchini  zuckerberg  zuni  zuppa  échezeaux  \n",
       "0      0        0     0         0           0     0      0          0  \n",
       "1      0        0     0         0           0     0      0          0  \n",
       "2      0        0     0         0           0     0      0          0  \n",
       "\n",
       "[3 rows x 15636 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### We will see what the data looks like and for this we need to convert it to a dataframe-\n",
    "\n",
    "text_count_df = pd.DataFrame(text_count.toarray(), columns= vect.get_feature_names())\n",
    "text_count_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "the     7929\n",
       "and     4425\n",
       "to      3826\n",
       "of      3776\n",
       "in      2820\n",
       "it      1933\n",
       "that    1726\n",
       "for     1444\n",
       "is      1373\n",
       "with    1352\n",
       "on      1108\n",
       "at      1026\n",
       "as       836\n",
       "from     728\n",
       "but      721\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Frequency analyisis-\n",
    "\n",
    "## For this, we need to sum up the columns to get frequency and sort it\n",
    "\n",
    "word_count = text_count_df.sum(axis= 0).sort_values(ascending = False)\n",
    "word_count[:15]   ## Printing the most frequent 15 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Removing Stopwords-\n",
    "\n",
    "## First we will remove basic stopwords-\n",
    "\n",
    "stop_words_1 = list(stopwords.words('english'))  ## Creating our 1st stopwords object\n",
    "\n",
    "vect_1 = CountVectorizer(stop_words = stop_words_1) ## Initializing the object for our countVector\n",
    "\n",
    "## Creating the representation or applying it\n",
    "\n",
    "text_count_1 = vect_1.fit_transform(df_text['split_final'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>000</th>\n",
       "      <th>0007</th>\n",
       "      <th>0071</th>\n",
       "      <th>0072</th>\n",
       "      <th>01</th>\n",
       "      <th>0100</th>\n",
       "      <th>0109when</th>\n",
       "      <th>015</th>\n",
       "      <th>017</th>\n",
       "      <th>0181</th>\n",
       "      <th>...</th>\n",
       "      <th>zip</th>\n",
       "      <th>zone</th>\n",
       "      <th>zones</th>\n",
       "      <th>zoraida</th>\n",
       "      <th>zour</th>\n",
       "      <th>zucchini</th>\n",
       "      <th>zuckerberg</th>\n",
       "      <th>zuni</th>\n",
       "      <th>zuppa</th>\n",
       "      <th>échezeaux</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 15505 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   000  0007  0071  0072  01  0100  0109when  015  017  0181  ...  zip  zone  \\\n",
       "0    1     0     0     0   0     0         0    0    0     0  ...    0     1   \n",
       "1    0     0     0     0   0     0         0    0    0     0  ...    0     0   \n",
       "2    0     0     0     0   0     0         0    0    0     0  ...    0     0   \n",
       "\n",
       "   zones  zoraida  zour  zucchini  zuckerberg  zuni  zuppa  échezeaux  \n",
       "0      0        0     0         0           0     0      0          0  \n",
       "1      0        0     0         0           0     0      0          0  \n",
       "2      0        0     0         0           0     0      0          0  \n",
       "\n",
       "[3 rows x 15505 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### We will see what the data looks like and for this we need to convert it to a dataframe-\n",
    "\n",
    "text_count_df_1 = pd.DataFrame(text_count_1.toarray(), columns= vect_1.get_feature_names())\n",
    "text_count_df_1.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "wa           626\n",
       "thi          509\n",
       "like         474\n",
       "ha           444\n",
       "said         423\n",
       "food         387\n",
       "san          384\n",
       "one          383\n",
       "restaur      359\n",
       "hi           355\n",
       "make         331\n",
       "wine         328\n",
       "francisco    321\n",
       "year         319\n",
       "new          304\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Frequency analyisis-\n",
    "\n",
    "## For this, we need to sum up the columns to get frequency and sort it\n",
    "\n",
    "word_count_1 = text_count_df_1.sum(axis= 0).sort_values(ascending = False)\n",
    "word_count_1[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Now, we will extend our Stopwords with 'like', 'said' and 'year'\n",
    "\n",
    "stop_words_2 = stop_words_1 + ['like', 'said', 'year']  ## Creating our 1st stopwords object\n",
    "\n",
    "vect_2 = CountVectorizer(stop_words = stop_words_2) ## Initializing the object for our countVector\n",
    "\n",
    "## Creating the representation or applying it\n",
    "\n",
    "text_count_2 = vect_2.fit_transform(df_text['split_final'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>000</th>\n",
       "      <th>0007</th>\n",
       "      <th>0071</th>\n",
       "      <th>0072</th>\n",
       "      <th>01</th>\n",
       "      <th>0100</th>\n",
       "      <th>0109when</th>\n",
       "      <th>015</th>\n",
       "      <th>017</th>\n",
       "      <th>0181</th>\n",
       "      <th>...</th>\n",
       "      <th>zip</th>\n",
       "      <th>zone</th>\n",
       "      <th>zones</th>\n",
       "      <th>zoraida</th>\n",
       "      <th>zour</th>\n",
       "      <th>zucchini</th>\n",
       "      <th>zuckerberg</th>\n",
       "      <th>zuni</th>\n",
       "      <th>zuppa</th>\n",
       "      <th>échezeaux</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 15502 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   000  0007  0071  0072  01  0100  0109when  015  017  0181  ...  zip  zone  \\\n",
       "0    1     0     0     0   0     0         0    0    0     0  ...    0     1   \n",
       "1    0     0     0     0   0     0         0    0    0     0  ...    0     0   \n",
       "2    0     0     0     0   0     0         0    0    0     0  ...    0     0   \n",
       "\n",
       "   zones  zoraida  zour  zucchini  zuckerberg  zuni  zuppa  échezeaux  \n",
       "0      0        0     0         0           0     0      0          0  \n",
       "1      0        0     0         0           0     0      0          0  \n",
       "2      0        0     0         0           0     0      0          0  \n",
       "\n",
       "[3 rows x 15502 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### We will see what the data looks like and for this we need to convert it to a dataframe-\n",
    "\n",
    "text_count_df_2 = pd.DataFrame(text_count_2.toarray(), columns= vect_2.get_feature_names())\n",
    "text_count_df_2.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "wa           626\n",
       "thi          509\n",
       "ha           444\n",
       "food         387\n",
       "san          384\n",
       "one          383\n",
       "restaur      359\n",
       "hi           355\n",
       "make         331\n",
       "wine         328\n",
       "francisco    321\n",
       "new          304\n",
       "would        258\n",
       "time         256\n",
       "also         255\n",
       "dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Frequency analyisis-\n",
    "\n",
    "## For this, we need to sum up the columns to get frequency and sort it\n",
    "\n",
    "word_count_2 = text_count_df_2.sum(axis= 0).sort_values(ascending = False)\n",
    "word_count_2[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Now, we will extend our Stopwords even further with 'would', and 'also'\n",
    "\n",
    "stop_words_3 = stop_words_2 + ['would', 'also']  ## Creating our 1st stopwords object\n",
    "\n",
    "vect_3 = CountVectorizer(stop_words = stop_words_3) ## Initializing the object for our countVector\n",
    "\n",
    "## Creating the representation or applying it\n",
    "\n",
    "text_count_3 = vect_3.fit_transform(df_text['split_final'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>000</th>\n",
       "      <th>0007</th>\n",
       "      <th>0071</th>\n",
       "      <th>0072</th>\n",
       "      <th>01</th>\n",
       "      <th>0100</th>\n",
       "      <th>0109when</th>\n",
       "      <th>015</th>\n",
       "      <th>017</th>\n",
       "      <th>0181</th>\n",
       "      <th>...</th>\n",
       "      <th>zip</th>\n",
       "      <th>zone</th>\n",
       "      <th>zones</th>\n",
       "      <th>zoraida</th>\n",
       "      <th>zour</th>\n",
       "      <th>zucchini</th>\n",
       "      <th>zuckerberg</th>\n",
       "      <th>zuni</th>\n",
       "      <th>zuppa</th>\n",
       "      <th>échezeaux</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 15500 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   000  0007  0071  0072  01  0100  0109when  015  017  0181  ...  zip  zone  \\\n",
       "0    1     0     0     0   0     0         0    0    0     0  ...    0     1   \n",
       "1    0     0     0     0   0     0         0    0    0     0  ...    0     0   \n",
       "2    0     0     0     0   0     0         0    0    0     0  ...    0     0   \n",
       "\n",
       "   zones  zoraida  zour  zucchini  zuckerberg  zuni  zuppa  échezeaux  \n",
       "0      0        0     0         0           0     0      0          0  \n",
       "1      0        0     0         0           0     0      0          0  \n",
       "2      0        0     0         0           0     0      0          0  \n",
       "\n",
       "[3 rows x 15500 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### We will see what the data looks like and for this we need to convert it to a dataframe-\n",
    "\n",
    "text_count_df_3 = pd.DataFrame(text_count_3.toarray(), columns= vect_3.get_feature_names())\n",
    "text_count_df_3.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 15 most frequent words after 2 iterations of removing various stopwords are:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "wa           626\n",
       "thi          509\n",
       "ha           444\n",
       "food         387\n",
       "san          384\n",
       "one          383\n",
       "restaur      359\n",
       "hi           355\n",
       "make         331\n",
       "wine         328\n",
       "francisco    321\n",
       "new          304\n",
       "time         256\n",
       "th           242\n",
       "bar          234\n",
       "dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Frequency analyisis-\n",
    "\n",
    "## For this, we need to sum up the columns to get frequency and sort it\n",
    "\n",
    "word_count_3 = text_count_df_3.sum(axis= 0).sort_values(ascending = False)\n",
    "print('The 15 most frequent words after 2 iterations of removing various stopwords are:')\n",
    "word_count_3[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **In the second step, perform topic modeling on the data with specifying three topics to be extracted. Based on looking at the top 15 words from each topic, can you identify a connection between the three original news categories and the three extracted topics?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Performing TopicModelling-\n",
    "\n",
    "## As we already have created CountVectorizer previously, we can now use that, but for this we need the following library-\n",
    "\n",
    "from sklearn.decomposition import LatentDirichletAllocation  ## Importing the library\n",
    "\n",
    "## We first create and object where we will specify the expected output. In this case, we already know that we expect \n",
    " # total 03 topics and that is our n_components.\n",
    "LDA = LatentDirichletAllocation(n_components = 3, random_state = 42)\n",
    "\n",
    "LDA_results = LDA.fit_transform(text_count_1) ## Fitting the CountVectorizer where we removed usual stopwords for the 1st time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[16.39551457,  0.33334586,  0.33333663, ...,  0.33333964,\n",
       "         0.3333365 ,  0.33333891],\n",
       "       [24.12241794,  0.46010968,  1.33332748, ...,  0.33333663,\n",
       "         1.33332793,  2.33332432],\n",
       "       [16.48206749,  1.20654446,  0.33333589, ...,  4.33332373,\n",
       "         0.33333557,  0.33333678]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Checking the results-\n",
    "\n",
    "## We can now check the results that are stored in an array. And we print only the components.\n",
    "\n",
    "LDA.components_\n",
    "\n",
    "## Here each row represents each topic and the there is a value for each word that indicates how the word is related to \n",
    " # the topic. The higher the values are, the more important or more characterized they are (in terms of what the text that \n",
    " # belongs to the topic it describes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['yellow', 'prolifer', 'sayr', 'altaca', 'statesla', 'thickest', 'newcom', 'percentil', 'hibernation', 'wrap', 'haa', 'thaiideavegetariansf', 'trung', 'wafer', 'saint']\n",
      "['saint', 'winemak', 'stabilizing', 'yellow', 'haa', 'ones', 'barack', 'hibernation', 'franciscocotognacotogna', 'thickest', 'foodcentr', 'sanchez', 'likely', 'wafer', 'restaurants']\n",
      "['wateri', 'user', 'sanchez', 'timeline', 'beermast', 'cookbook', 'foodcentr', 'cup1', 'ones', 'maker', 'haa', 'wafer', 'likely', 'winemak', 'thickest']\n"
     ]
    }
   ],
   "source": [
    "### Printing the top 15 words from each topic-\n",
    "\n",
    "for topic, component in enumerate(LDA.components_):\n",
    "    words_sorted = np.argsort(component)[-15:]  ## Sorting in ascending order and commanding for top 15 \n",
    "    \n",
    "    print([vect_3.get_feature_names()[i] for i in words_sorted])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.47972417e-01, 7.69220112e-04, 7.51258363e-01],\n",
       "       [9.97477495e-01, 1.25802466e-03, 1.26448058e-03],\n",
       "       [9.96425727e-01, 1.84534017e-03, 1.72893316e-03],\n",
       "       [4.78380795e-01, 1.25363056e-03, 5.20365574e-01],\n",
       "       [1.39908894e-03, 1.03287585e-01, 8.95313326e-01],\n",
       "       [2.45681751e-01, 5.48251349e-01, 2.06066900e-01],\n",
       "       [7.81126883e-01, 9.22404256e-04, 2.17950713e-01],\n",
       "       [6.07962179e-01, 3.85824216e-01, 6.21360536e-03],\n",
       "       [9.17598254e-04, 9.31052978e-04, 9.98151349e-01],\n",
       "       [9.98880053e-01, 5.62567122e-04, 5.57379382e-04],\n",
       "       [3.91439393e-01, 6.07832285e-01, 7.28322329e-04],\n",
       "       [6.00146722e-01, 8.58326258e-04, 3.98994952e-01],\n",
       "       [9.97340857e-01, 1.33972223e-03, 1.31942102e-03],\n",
       "       [3.96479091e-01, 6.02633219e-01, 8.87689676e-04],\n",
       "       [9.97183544e-01, 1.43696063e-03, 1.37949551e-03],\n",
       "       [2.30446501e-01, 7.68728139e-01, 8.25360269e-04],\n",
       "       [2.04240897e-01, 7.94731286e-01, 1.02781767e-03],\n",
       "       [9.25871726e-01, 7.32266552e-02, 9.01618796e-04],\n",
       "       [1.10155109e-03, 9.97913091e-01, 9.85358003e-04],\n",
       "       [9.98639461e-01, 6.79345912e-04, 6.81192768e-04],\n",
       "       [9.94612409e-01, 2.82357173e-03, 2.56401892e-03],\n",
       "       [9.98975864e-01, 5.13399956e-04, 5.10735912e-04],\n",
       "       [9.97977208e-01, 1.01860019e-03, 1.00419144e-03],\n",
       "       [9.98732173e-01, 6.38846884e-04, 6.28979868e-04],\n",
       "       [9.97559275e-01, 1.21605884e-03, 1.22466633e-03],\n",
       "       [4.10293646e-01, 1.25516490e-03, 5.88451189e-01],\n",
       "       [3.35860592e-01, 1.00070737e-03, 6.63138701e-01],\n",
       "       [5.96139536e-01, 4.03404026e-01, 4.56438216e-04],\n",
       "       [7.34053174e-03, 9.85462888e-01, 7.19658062e-03],\n",
       "       [9.96724109e-01, 1.66104471e-03, 1.61484601e-03],\n",
       "       [4.75348538e-01, 7.77575458e-04, 5.23873886e-01],\n",
       "       [1.98404896e-01, 8.00581669e-01, 1.01343499e-03],\n",
       "       [9.98919316e-01, 5.37822628e-04, 5.42861676e-04],\n",
       "       [4.91953148e-02, 9.49561873e-01, 1.24281252e-03],\n",
       "       [9.97925955e-01, 1.04974077e-03, 1.02430385e-03],\n",
       "       [9.97783364e-01, 1.10780713e-03, 1.10882894e-03],\n",
       "       [9.96849709e-01, 1.59025047e-03, 1.56004052e-03],\n",
       "       [9.98933356e-01, 5.33403508e-04, 5.33240848e-04],\n",
       "       [1.78262019e-03, 9.96604578e-01, 1.61280160e-03],\n",
       "       [9.91198308e-01, 4.46363256e-03, 4.33805896e-03],\n",
       "       [9.98101656e-01, 9.62889561e-04, 9.35454689e-04],\n",
       "       [3.37507533e-01, 6.61928556e-01, 5.63910788e-04],\n",
       "       [4.12853388e-01, 8.94290187e-04, 5.86252322e-01],\n",
       "       [8.43935024e-01, 3.28163549e-03, 1.52783341e-01],\n",
       "       [9.98532807e-01, 7.31756235e-04, 7.35436954e-04],\n",
       "       [2.75007515e-02, 9.71692724e-01, 8.06524499e-04],\n",
       "       [4.58511674e-03, 9.91658948e-01, 3.75593484e-03],\n",
       "       [4.73184624e-04, 9.99039959e-01, 4.86856789e-04],\n",
       "       [9.33056433e-04, 9.98078631e-01, 9.88312789e-04],\n",
       "       [4.77010241e-04, 9.99037530e-01, 4.85459811e-04],\n",
       "       [7.77791021e-04, 5.86072108e-01, 4.13150101e-01],\n",
       "       [6.58886491e-04, 6.85451695e-04, 9.98655662e-01],\n",
       "       [4.32686221e-04, 9.99112040e-01, 4.55273969e-04],\n",
       "       [4.84540318e-04, 4.59057729e-02, 9.53609687e-01],\n",
       "       [7.68471157e-01, 1.82628246e-01, 4.89005971e-02],\n",
       "       [5.26300310e-04, 5.53303273e-04, 9.98920396e-01],\n",
       "       [4.53460269e-01, 7.63798633e-04, 5.45775933e-01],\n",
       "       [2.38513002e-04, 9.99510896e-01, 2.50591379e-04],\n",
       "       [7.48218203e-04, 8.10995814e-04, 9.98440786e-01],\n",
       "       [9.91459668e-04, 1.02436992e-03, 9.97984170e-01],\n",
       "       [7.77984653e-04, 7.95838902e-04, 9.98426176e-01],\n",
       "       [4.43736001e-04, 4.55934408e-04, 9.99100330e-01],\n",
       "       [1.46791567e-03, 9.96909968e-01, 1.62211643e-03],\n",
       "       [1.05442948e-03, 1.11482217e-03, 9.97830748e-01],\n",
       "       [9.89168254e-01, 5.79314195e-04, 1.02524321e-02],\n",
       "       [4.18786180e-04, 9.99122678e-01, 4.58535348e-04],\n",
       "       [8.41331618e-04, 2.97604633e-01, 7.01554035e-01],\n",
       "       [1.05051752e-03, 1.11326364e-03, 9.97836219e-01],\n",
       "       [6.39579567e-04, 9.98683525e-01, 6.76895866e-04],\n",
       "       [1.61314365e-03, 9.79062067e-02, 9.00480650e-01],\n",
       "       [5.67399687e-04, 5.52641578e-04, 9.98879959e-01],\n",
       "       [1.83146039e-04, 9.99621758e-01, 1.95095701e-04],\n",
       "       [6.16712841e-04, 6.33360584e-04, 9.98749927e-01],\n",
       "       [4.01685906e-04, 9.99177006e-01, 4.21308515e-04],\n",
       "       [6.08942541e-01, 3.90564499e-01, 4.92959928e-04],\n",
       "       [1.58337015e-03, 1.84017222e-01, 8.14399408e-01],\n",
       "       [3.93896559e-04, 9.99201403e-01, 4.04700728e-04],\n",
       "       [1.21926748e-03, 9.97614053e-01, 1.16667907e-03],\n",
       "       [3.34546664e-01, 1.93278113e-01, 4.72175222e-01],\n",
       "       [4.59562766e-04, 4.93514328e-04, 9.99046923e-01],\n",
       "       [7.66366829e-04, 9.98551098e-01, 6.82534989e-04],\n",
       "       [5.88673793e-04, 9.28918642e-02, 9.06519462e-01],\n",
       "       [3.64361415e-04, 3.85081037e-04, 9.99250558e-01],\n",
       "       [4.08728195e-01, 5.89733258e-01, 1.53854753e-03],\n",
       "       [9.52956526e-04, 1.04235064e-03, 9.98004693e-01],\n",
       "       [4.81861476e-04, 3.46197258e-01, 6.53320881e-01],\n",
       "       [2.11837574e-01, 8.39178232e-04, 7.87323248e-01],\n",
       "       [2.43486658e-04, 9.03620129e-01, 9.61363839e-02],\n",
       "       [5.55004793e-04, 9.98843564e-01, 6.01430827e-04],\n",
       "       [7.18983990e-04, 7.47039065e-04, 9.98533977e-01],\n",
       "       [7.76816172e-04, 7.54337268e-04, 9.98468847e-01],\n",
       "       [4.25084706e-04, 9.99102090e-01, 4.72825385e-04],\n",
       "       [4.19748027e-04, 4.35557938e-04, 9.99144694e-01],\n",
       "       [9.85875758e-04, 3.90312650e-01, 6.08701474e-01],\n",
       "       [6.15401371e-01, 3.83204978e-01, 1.39365124e-03],\n",
       "       [5.55849553e-01, 3.86170200e-01, 5.79802472e-02],\n",
       "       [3.08747570e-01, 6.21529803e-01, 6.97226272e-02],\n",
       "       [2.09702843e-03, 1.77681717e-03, 9.96126154e-01],\n",
       "       [2.69965205e-04, 9.99451290e-01, 2.78745294e-04],\n",
       "       [6.42225223e-04, 6.65849001e-04, 9.98691926e-01],\n",
       "       [3.47112777e-02, 9.87944288e-04, 9.64300778e-01],\n",
       "       [4.34877391e-04, 9.56879389e-01, 4.26857338e-02],\n",
       "       [7.67882946e-04, 8.71626468e-04, 9.98360491e-01],\n",
       "       [5.03785293e-01, 2.47552454e-01, 2.48662253e-01],\n",
       "       [5.82517545e-04, 6.16345469e-04, 9.98801137e-01],\n",
       "       [3.49175902e-04, 9.99247571e-01, 4.03253140e-04],\n",
       "       [2.93273515e-04, 9.99392728e-01, 3.13998172e-04],\n",
       "       [3.49125509e-01, 6.50310381e-01, 5.64109879e-04],\n",
       "       [5.75518298e-04, 5.83639380e-04, 9.98840842e-01],\n",
       "       [1.18308168e-03, 2.63381670e-01, 7.35435248e-01],\n",
       "       [6.95476645e-04, 7.59060101e-04, 9.98545463e-01],\n",
       "       [5.05951297e-01, 4.88479053e-01, 5.56964977e-03],\n",
       "       [4.71762725e-04, 5.00471145e-04, 9.99027766e-01],\n",
       "       [5.62857889e-04, 5.98049623e-04, 9.98839092e-01],\n",
       "       [6.53185973e-04, 3.26373019e-01, 6.72973795e-01],\n",
       "       [2.89438258e-04, 9.99430972e-01, 2.79589381e-04],\n",
       "       [1.88510680e-04, 9.99604301e-01, 2.07188679e-04],\n",
       "       [5.28808174e-04, 1.16198757e-02, 9.87851316e-01],\n",
       "       [4.44838728e-04, 9.81554171e-01, 1.80009903e-02],\n",
       "       [2.18888516e-01, 8.27950972e-04, 7.80283533e-01],\n",
       "       [1.14196412e-03, 1.17546535e-03, 9.97682571e-01],\n",
       "       [1.82391626e-01, 8.16653313e-01, 9.55061367e-04],\n",
       "       [5.51940810e-04, 9.98869503e-01, 5.78556465e-04],\n",
       "       [6.41113944e-01, 3.58541919e-01, 3.44136705e-04],\n",
       "       [3.22350584e-04, 9.99348639e-01, 3.29010825e-04],\n",
       "       [1.42041202e-03, 9.97121469e-01, 1.45811931e-03],\n",
       "       [9.98413927e-01, 8.12941569e-04, 7.73131319e-04],\n",
       "       [5.39835111e-01, 4.59487120e-01, 6.77769057e-04],\n",
       "       [4.81383622e-04, 9.99018409e-01, 5.00207115e-04],\n",
       "       [3.97508755e-04, 6.11791802e-01, 3.87810689e-01],\n",
       "       [6.15485043e-04, 6.59692271e-04, 9.98724823e-01],\n",
       "       [5.48484792e-04, 9.98919350e-01, 5.32165025e-04],\n",
       "       [1.85333978e-03, 1.90211469e-03, 9.96244546e-01],\n",
       "       [1.83289219e-03, 1.89091377e-03, 9.96276194e-01],\n",
       "       [5.75444905e-04, 9.98825284e-01, 5.99271530e-04],\n",
       "       [3.61357456e-04, 9.99279945e-01, 3.58697191e-04],\n",
       "       [3.27776971e-04, 3.37952886e-04, 9.99334270e-01],\n",
       "       [1.12643899e-03, 1.21925182e-03, 9.97654309e-01],\n",
       "       [6.72582257e-01, 6.64766689e-02, 2.60941074e-01],\n",
       "       [2.82149364e-03, 2.88945392e-03, 9.94289052e-01],\n",
       "       [3.00347799e-04, 3.06070627e-04, 9.99393582e-01],\n",
       "       [3.31891069e-01, 6.66201698e-01, 1.90723272e-03],\n",
       "       [1.96514504e-01, 9.94657331e-04, 8.02490838e-01],\n",
       "       [8.88688885e-04, 1.08640708e-01, 8.90470604e-01],\n",
       "       [3.90694341e-01, 4.80139661e-01, 1.29165998e-01],\n",
       "       [4.57980460e-04, 6.93970199e-01, 3.05571821e-01],\n",
       "       [3.53689690e-04, 3.85322149e-04, 9.99260988e-01],\n",
       "       [1.13483754e-03, 9.97672818e-01, 1.19234447e-03],\n",
       "       [4.67471021e-04, 4.19715044e-01, 5.79817485e-01],\n",
       "       [3.68519079e-04, 9.99215427e-01, 4.16053915e-04],\n",
       "       [8.73852839e-04, 9.68087504e-04, 9.98158060e-01],\n",
       "       [1.00712151e-03, 9.98028688e-01, 9.64190583e-04],\n",
       "       [3.19281936e-04, 9.99356013e-01, 3.24705496e-04],\n",
       "       [6.47557288e-02, 8.27869519e-04, 9.34416402e-01],\n",
       "       [7.76204340e-04, 9.98477031e-01, 7.46765003e-04],\n",
       "       [4.08999932e-04, 9.99155975e-01, 4.35025040e-04],\n",
       "       [3.25552525e-04, 9.99333377e-01, 3.41070113e-04],\n",
       "       [5.56042314e-04, 3.51825565e-01, 6.47618393e-01],\n",
       "       [7.67928570e-04, 8.24759729e-04, 9.98407312e-01],\n",
       "       [5.49669986e-04, 9.93361142e-01, 6.08918844e-03],\n",
       "       [1.11226006e-03, 9.97775241e-01, 1.11249892e-03],\n",
       "       [2.23338064e-03, 9.95754405e-01, 2.01221448e-03],\n",
       "       [1.15159163e-03, 1.21153356e-03, 9.97636875e-01],\n",
       "       [1.06084527e-03, 9.97869925e-01, 1.06923020e-03],\n",
       "       [5.87172170e-04, 6.61537480e-04, 9.98751290e-01]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### We can check here what each row tell us in what extent each text belongs to each topic\n",
    "\n",
    "LDA_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 0, 0, 2, 2, 1, 0, 0, 2, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0,\n",
       "       0, 0, 0, 2, 2, 0, 1, 0, 2, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 2, 0,\n",
       "       0, 1, 1, 1, 1, 1, 1, 2, 1, 2, 0, 2, 2, 1, 2, 2, 2, 2, 1, 2, 0, 1,\n",
       "       2, 2, 1, 2, 2, 1, 2, 1, 0, 2, 1, 1, 2, 2, 1, 2, 2, 1, 2, 2, 2, 1,\n",
       "       1, 2, 2, 1, 2, 2, 0, 0, 1, 2, 1, 2, 2, 1, 2, 0, 2, 1, 1, 1, 2, 2,\n",
       "       2, 0, 2, 2, 2, 1, 1, 2, 1, 2, 2, 1, 1, 0, 1, 1, 0, 0, 1, 1, 2, 1,\n",
       "       2, 2, 1, 1, 2, 2, 0, 2, 2, 1, 2, 2, 1, 1, 2, 1, 2, 1, 2, 1, 1, 2,\n",
       "       1, 1, 1, 2, 2, 1, 1, 1, 2, 1, 2], dtype=int64)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### We can simply check which number is the higest, and will assign that topic-\n",
    "\n",
    "LDA_results.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>title</th>\n",
       "      <th>category</th>\n",
       "      <th>stemmed</th>\n",
       "      <th>split_final</th>\n",
       "      <th>Topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Feb. 13, 2019</td>\n",
       "      <td>while millions more pacific gas and electric c...</td>\n",
       "      <td>PG&amp;E says it’s still trying to limit power shu...</td>\n",
       "      <td>Biz+Tech</td>\n",
       "      <td>[while, million, more, pacif, ga, and, electr,...</td>\n",
       "      <td>while million more pacif ga and electr co. cus...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Feb. 13, 2019</td>\n",
       "      <td>singapore (ap) — asian stocks were mostly lowe...</td>\n",
       "      <td>Asian shares retreat as China, US begin trade ...</td>\n",
       "      <td>Biz+Tech</td>\n",
       "      <td>[singapor, (ap), —, asian, stock, were, mostli...</td>\n",
       "      <td>singapor (ap) — asian stock were mostli lower ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Feb. 16, 2018</td>\n",
       "      <td>if anyone knew the ropes about airbnb rentals,...</td>\n",
       "      <td>Long, winding road to SF’s get-tough registrat...</td>\n",
       "      <td>Biz+Tech</td>\n",
       "      <td>[if, anyon, knew, the, rope, about, airbnb, re...</td>\n",
       "      <td>if anyon knew the rope about airbnb rentals, d...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Feb. 11, 2019</td>\n",
       "      <td>half of the pg&amp;e corp. board of directors will...</td>\n",
       "      <td>PG&amp;E to replace half of its board in wake of b...</td>\n",
       "      <td>Biz+Tech</td>\n",
       "      <td>[half, of, the, pg&amp;e, corp., board, of, direct...</td>\n",
       "      <td>half of the pg&amp;e corp. board of director will ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Feb. 15, 2019</td>\n",
       "      <td>a: real estate today moves at the speed of inf...</td>\n",
       "      <td>What’s a piece of technology you can’t imagine...</td>\n",
       "      <td>Biz+Tech</td>\n",
       "      <td>[a:, real, estat, today, move, at, the, speed,...</td>\n",
       "      <td>a: real estat today move at the speed of infor...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>Aug. 24, 2017</td>\n",
       "      <td>montesacro pinseriaroman-style pinsa, a type o...</td>\n",
       "      <td>The best restaurants in San Francisco’s Theate...</td>\n",
       "      <td>Food</td>\n",
       "      <td>[montesacro, pinseriaroman-styl, pinsa,, a, ty...</td>\n",
       "      <td>montesacro pinseriaroman-styl pinsa, a type of...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>Feb. 15, 2019</td>\n",
       "      <td>there’s change on the horizon for sustainable ...</td>\n",
       "      <td>Belcampo to close Russian Hill and Palo Alto o...</td>\n",
       "      <td>Food</td>\n",
       "      <td>[there’, chang, on, the, horizon, for, sustain...</td>\n",
       "      <td>there’ chang on the horizon for sustain meat s...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>Jan. 19, 2017</td>\n",
       "      <td>hawaiian macaroni &amp; cheeseserves 12-14this rec...</td>\n",
       "      <td>Recipe: Hawaiian Macaroni &amp; Cheese</td>\n",
       "      <td>Food</td>\n",
       "      <td>[hawaiian, macaroni, &amp;, cheeseserv, 12-14thi, ...</td>\n",
       "      <td>hawaiian macaroni &amp; cheeseserv 12-14thi recip ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>Feb. 8, 2019</td>\n",
       "      <td>nigel jones knows the soft, yellowish flesh of...</td>\n",
       "      <td>Calabash: An Afro-Caribbean, Malaysian and Per...</td>\n",
       "      <td>Food</td>\n",
       "      <td>[nigel, jone, know, the, soft,, yellowish, fle...</td>\n",
       "      <td>nigel jone know the soft, yellowish flesh of t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>Dec. 13, 2018</td>\n",
       "      <td>when it opened two decades ago, tony gulisano’...</td>\n",
       "      <td>Recipe: How to make Chow’s famous Ginger Cake</td>\n",
       "      <td>Food</td>\n",
       "      <td>[when, it, open, two, decad, ago,, toni, gulis...</td>\n",
       "      <td>when it open two decad ago, toni gulisano’ cas...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>165 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              date                                               text  \\\n",
       "0    Feb. 13, 2019  while millions more pacific gas and electric c...   \n",
       "1    Feb. 13, 2019  singapore (ap) — asian stocks were mostly lowe...   \n",
       "2    Feb. 16, 2018  if anyone knew the ropes about airbnb rentals,...   \n",
       "3    Feb. 11, 2019  half of the pg&e corp. board of directors will...   \n",
       "4    Feb. 15, 2019  a: real estate today moves at the speed of inf...   \n",
       "..             ...                                                ...   \n",
       "160  Aug. 24, 2017  montesacro pinseriaroman-style pinsa, a type o...   \n",
       "161  Feb. 15, 2019  there’s change on the horizon for sustainable ...   \n",
       "162  Jan. 19, 2017  hawaiian macaroni & cheeseserves 12-14this rec...   \n",
       "163   Feb. 8, 2019  nigel jones knows the soft, yellowish flesh of...   \n",
       "164  Dec. 13, 2018  when it opened two decades ago, tony gulisano’...   \n",
       "\n",
       "                                                 title  category  \\\n",
       "0    PG&E says it’s still trying to limit power shu...  Biz+Tech   \n",
       "1    Asian shares retreat as China, US begin trade ...  Biz+Tech   \n",
       "2    Long, winding road to SF’s get-tough registrat...  Biz+Tech   \n",
       "3    PG&E to replace half of its board in wake of b...  Biz+Tech   \n",
       "4    What’s a piece of technology you can’t imagine...  Biz+Tech   \n",
       "..                                                 ...       ...   \n",
       "160  The best restaurants in San Francisco’s Theate...      Food   \n",
       "161  Belcampo to close Russian Hill and Palo Alto o...      Food   \n",
       "162                 Recipe: Hawaiian Macaroni & Cheese      Food   \n",
       "163  Calabash: An Afro-Caribbean, Malaysian and Per...      Food   \n",
       "164      Recipe: How to make Chow’s famous Ginger Cake      Food   \n",
       "\n",
       "                                               stemmed  \\\n",
       "0    [while, million, more, pacif, ga, and, electr,...   \n",
       "1    [singapor, (ap), —, asian, stock, were, mostli...   \n",
       "2    [if, anyon, knew, the, rope, about, airbnb, re...   \n",
       "3    [half, of, the, pg&e, corp., board, of, direct...   \n",
       "4    [a:, real, estat, today, move, at, the, speed,...   \n",
       "..                                                 ...   \n",
       "160  [montesacro, pinseriaroman-styl, pinsa,, a, ty...   \n",
       "161  [there’, chang, on, the, horizon, for, sustain...   \n",
       "162  [hawaiian, macaroni, &, cheeseserv, 12-14thi, ...   \n",
       "163  [nigel, jone, know, the, soft,, yellowish, fle...   \n",
       "164  [when, it, open, two, decad, ago,, toni, gulis...   \n",
       "\n",
       "                                           split_final  Topic  \n",
       "0    while million more pacif ga and electr co. cus...      2  \n",
       "1    singapor (ap) — asian stock were mostli lower ...      0  \n",
       "2    if anyon knew the rope about airbnb rentals, d...      0  \n",
       "3    half of the pg&e corp. board of director will ...      2  \n",
       "4    a: real estat today move at the speed of infor...      2  \n",
       "..                                                 ...    ...  \n",
       "160  montesacro pinseriaroman-styl pinsa, a type of...      1  \n",
       "161  there’ chang on the horizon for sustain meat s...      1  \n",
       "162  hawaiian macaroni & cheeseserv 12-14thi recip ...      2  \n",
       "163  nigel jone know the soft, yellowish flesh of t...      1  \n",
       "164  when it open two decad ago, toni gulisano’ cas...      2  \n",
       "\n",
       "[165 rows x 7 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### We will now add this as a new column to the dataset\n",
    "\n",
    "df_text['Topic'] = LDA_results.argmax(axis=1)\n",
    "df_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### We can see a difference between the three original news categories and the three extracted topics as some of them are\n",
    "  # wrongly classified. It may happen due to Stemming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
